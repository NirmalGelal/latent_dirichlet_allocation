{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c50799a",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "- LDA\n",
    "- topic modeling algorithm (statistical method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd549ae",
   "metadata": {},
   "source": [
    "## - Fetches the data names as 20newsgroup from sklearn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd09e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d94ee5",
   "metadata": {},
   "source": [
    "## - Storing the fetched data into corpus_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81c45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_temp = newsgroups_train.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1dbb03",
   "metadata": {},
   "source": [
    "# - Manual pre-processing techniques\n",
    " *Commented out because it is too time consuming for large dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5012815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from string import punctuation\n",
    "# from gensim import corpora, models, similarities\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# # from nltk.stem import SnowballStemmer\n",
    "\n",
    "# # stemmer = SnowballStemmer('english')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stoplist = stopwords.words('english')\n",
    "# numbers_punc = [str(i) for i in range(10)] + list(punctuation)\n",
    "\n",
    "# preprocessed_corpus = []\n",
    "# i = 1\n",
    "# for sentence in corpus:\n",
    "#     word_list = sentence.lower().split()\n",
    "#     temp = []\n",
    "#     for word in word_list :\n",
    "#         if word not in stoplist and len(word)>3:\n",
    "#             letter_temp = []\n",
    "#             for letter in word:\n",
    "#                 if letter not in numbers_punc :\n",
    "                    \n",
    "#                         letter_temp.append(letter)\n",
    "#             if len(letter_temp)!=0 :   \n",
    "#                 temp.append(\"\".join(letter_temp))\n",
    "#             temp = [lemmatizer.lemmatize(word,pos='v') for word in temp]\n",
    "#     preprocessed_corpus.append(temp)\n",
    "#     print(i)\n",
    "#     i+=1\n",
    "\n",
    "# #initialize a dictionary (value,key)=(5,'elon') means word('elon') is repitated 5 times.\n",
    "# dictionary = corpora.Dictionary(preprocessed_corpus)\n",
    "# preprocessed_corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26577e5",
   "metadata": {},
   "source": [
    "## Automatic pre-processing using Gensim Library\n",
    "*Snowball Stemmer and Wordnet Lemmatizer from nltk library is used for stemming and lemmatization purpose. Stopwords are are removed and the length of words less than 3 are discarded.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f09379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "import gensim\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "    \n",
    "processed_docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "\n",
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e308d",
   "metadata": {},
   "source": [
    "## - Creating the dictionary and Filtering the extremes\n",
    "Creating the dictionary: We create the dictionary of (v:k) pair where v is the number of repitition of words and k is the word.\n",
    "\n",
    "Filtering the extremese: This is an optional step that removes the extreme less and extreme high frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfad1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 15 times\n",
    "- words appearing in more than 10% of all documents\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12038d3e",
   "metadata": {},
   "source": [
    "## - Document to BOW(Bag Of Words):\n",
    "We create (token_id, token_count) form for each document using doc2bow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543c0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document, (token_id,token_count) is maintained\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792822c8",
   "metadata": {},
   "source": [
    "## Creating LDA Model\n",
    "The common parameters used in LDA are:\n",
    "- num_topics: number of latent topics to be extracted from the corpus.\n",
    "- passes: number of training passes through the corpus\n",
    "- workers: number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "- id2word: mapping from word ids (integers) to words (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79271cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "lda_model = models.LdaMulticore(bow_corpus,\n",
    "                            num_topics = 8,\n",
    "                            id2word = dictionary,\n",
    "                            passes = 20,\n",
    "                            workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d619040",
   "metadata": {},
   "source": [
    "## Printing words occuring in the topic and its relative weight:\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab9cc839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"presid\" + 0.006*\"american\" + 0.005*\"govern\" + 0.004*\"clinton\" + 0.004*\"money\" + 0.004*\"nation\" + 0.004*\"health\" + 0.004*\"cleveland\" + 0.003*\"talk\" + 0.003*\"happen\"'),\n",
       " (1,\n",
       "  '0.015*\"game\" + 0.013*\"team\" + 0.010*\"space\" + 0.009*\"play\" + 0.009*\"nasa\" + 0.008*\"player\" + 0.006*\"hockey\" + 0.005*\"season\" + 0.005*\"toronto\" + 0.005*\"orbit\"'),\n",
       " (2,\n",
       "  '0.010*\"armenian\" + 0.008*\"israel\" + 0.007*\"kill\" + 0.007*\"isra\" + 0.006*\"govern\" + 0.006*\"turkish\" + 0.005*\"jew\" + 0.005*\"weapon\" + 0.005*\"arab\" + 0.004*\"crime\"'),\n",
       " (3,\n",
       "  '0.011*\"christian\" + 0.007*\"jesus\" + 0.006*\"exist\" + 0.005*\"moral\" + 0.004*\"bibl\" + 0.004*\"word\" + 0.004*\"religion\" + 0.004*\"life\" + 0.004*\"church\" + 0.004*\"evid\"'),\n",
       " (4,\n",
       "  '0.008*\"drive\" + 0.006*\"bike\" + 0.006*\"power\" + 0.005*\"wire\" + 0.005*\"engin\" + 0.004*\"car\" + 0.004*\"light\" + 0.004*\"speed\" + 0.003*\"turn\" + 0.003*\"littl\"'),\n",
       " (5,\n",
       "  '0.022*\"window\" + 0.020*\"file\" + 0.011*\"program\" + 0.010*\"drive\" + 0.008*\"card\" + 0.007*\"scsi\" + 0.007*\"version\" + 0.007*\"disk\" + 0.006*\"driver\" + 0.006*\"server\"'),\n",
       " (6,\n",
       "  '0.015*\"encrypt\" + 0.012*\"chip\" + 0.011*\"secur\" + 0.010*\"clipper\" + 0.008*\"govern\" + 0.008*\"public\" + 0.007*\"key\" + 0.005*\"escrow\" + 0.005*\"netcom\" + 0.005*\"privaci\"'),\n",
       " (7,\n",
       "  '0.007*\"softwar\" + 0.006*\"imag\" + 0.006*\"data\" + 0.006*\"avail\" + 0.005*\"access\" + 0.005*\"graphic\" + 0.005*\"sale\" + 0.005*\"list\" + 0.005*\"program\" + 0.005*\"appl\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1f088",
   "metadata": {},
   "source": [
    "## Interpreting results:\n",
    "- Topic 0: Politics\n",
    "    It contains word like president, american, govern, clinton,... This might be Politics.\n",
    "- Topic 1: Sports\n",
    "    It contains word like game, team, play, player,... This might be Sports.\n",
    "- Topic 2: Crime and Violence\n",
    "    It contains word like kill, weapon, crime,... This might be Violence.\n",
    "- Topic 3: Religion\n",
    "    It contains word like christian, jesus, bible, religion,... This might be Religion.\n",
    "- Topic 4: Automobile\n",
    "    It contains word like drive, bike, engine, car,... This might be Automobile.\n",
    "- Topic 5: Graphic Cards\n",
    "    It contains word like file, program, drive, card, version, driver,... This might be Graphic Cards. \n",
    "- Topic 6: Security\n",
    "    It contains word like encrypt, chip, secure, clipper, key, privacy,... This might be Security.\n",
    "- Topic 7: Technology\n",
    "    It contains words like software, image, data, program,... This might be Technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed728e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
